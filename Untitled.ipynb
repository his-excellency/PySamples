{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillupleads.py\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import ftplib as ftp\n",
    "import sys\n",
    "import codecs\n",
    "import time\n",
    "import pandas as pd\n",
    "import enum\n",
    "import pickle as pk\n",
    "from neo4jrestclient.client import GraphDatabase\n",
    "from neo4jrestclient import client\n",
    "import sklearn\n",
    "import operator\n",
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = GraphDatabase(\"http://localhost:7474\",username=\"neo4j\",password=\"pradeep\")\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_lead_substreetid = \"\"\"match (x:xref) - [:in] -> (s:substreet)<-[]-(n:npi)-[:business_address]->(badd:substreet)  \n",
    "where x.original_lead_data = 'True' and n.ind_or_org='2'\n",
    "return x.name, badd.substreetId,case x.status when \"A\" then 1 else 0 end, case x.rem_status when \"Active\" then 1 else 0 end,case x.trade_class_desc when \"RETAIL - INDEPENDENT\" then 1 else 0 end as labels\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_substreetid =np.unique( np.squeeze(np.array((db.query(q_lead_substreetid)))) , axis=0)\n",
    "lead_substreetid = np.unique(lead_substreetid[:,1:], axis=0)\n",
    "classlabels = lead_substreetid[:,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for getting all Taxonomies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q_All_Taxonomies = \"\"\"match (t:taxonomy) return distinct(t.classification) as s order by s\"\"\"\n",
    "\n",
    "All_Taxonomies = (np.squeeze(np.array(db.query(q_All_Taxonomies))))\n",
    "\n",
    "l = len(All_Taxonomies)\n",
    "\n",
    "tax_dict = {All_Taxonomies[i]:i for i in range(l)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for number of NPIs per taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qNpiTax =\"\"\"match  (badd:substreet) <- [:practice_address] - (n:npi) - [:npi_taxonomy] -> (t:taxonomy)\n",
    "where  badd.substreetId = '%s'\n",
    "with collect({name:t.classification, npiname:n.name}) as s1\n",
    "match  (badd:substreet) <- [:business_address] - (n:npi) - [:practice_address] -> (padd:substreet)\n",
    "where  badd.substreetId = '%s' and n.ind_or_org = '2'\n",
    "with padd, s1\n",
    "match (t:taxonomy)<- [:npi_taxonomy]-  (n:npi) - [:practice_address] -> (padd)\n",
    "with s1, collect({name:t.classification, npiname:n.name}) as s2\n",
    "unwind (s1 + s2) as rows\n",
    "return distinct(rows.name) as taxon, count(distinct rows.npiname) as ct\n",
    "order by taxon\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for number of practice addresses for a given business address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qNumPracAddr = \"\"\"match (badd:substreet)<-[:business_address]-(n:npi)-[:practice_address]->(padd:substreet) where badd.substreetId='%s' return count(padd)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "company_details=[]\n",
    "addrDict={}\n",
    "for substrid in lead_substreetid[:,0]:\n",
    "    if(substrid!=None):\n",
    "        c_d=l*[0]\n",
    "        Q1 = qNpiTax%(substrid,substrid)\n",
    "        Q2 = qNumPracAddr%(substrid)\n",
    "        addrDict[i]=substrid\n",
    "\n",
    "        NpiTax = list(db.query(Q1))\n",
    "        NumPracAddr = db.query(Q2)[0]\n",
    "        for k in NpiTax:\n",
    "            c_d[ tax_dict[ k[0] ] ]=k[1]\n",
    "\n",
    "        #feature_combined = np.array(NumPracAddr+list(lead_substreetid[i,1:3].astype(int))+c_d)\n",
    "        feature_combined = np.array(NumPracAddr+c_d)\n",
    "        company_details.append(feature_combined)\n",
    "        i+=1\n",
    "        print(100*(i/len(lead_substreetid[:,0])),end=\"\\r\")\n",
    "        #if(i==5):break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*(i)/len(lead_substreetid[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kjkjk.pickle','wb') as handle:\n",
    "    pk.dump((tax_dict, addrDict,company_details,classlabels),handle,protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('opopop.pickle','rb') as handle:\n",
    "    leadData = pk.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the lables\n",
    "when there are large number of npis change that to non retail independent\n",
    "go through every case of retail independent (1)(pharmacy not owned by changed) and if the threshold is >20 make it 0\n",
    "\n",
    "randomly select npi businesses\n",
    "remove 30% of retail independent and use that in test data\n",
    "\n",
    "features for the business, customer type, sales history\n",
    "\n",
    "urban zipcodes ---might pick retail independence--check if every retail independence \n",
    "get sub address of retail independent independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxdict = leadData[0]\n",
    "\n",
    "tpharm=[t for t in taxdict.keys() if 'Pharma' in t ]\n",
    "\n",
    "pharmIndx=[taxdict[i] for i in tpharm]\n",
    "nonpharmIndx=[i for i in range(len(taxdict)) if i not in pharmIndx]\n",
    "\n",
    "leadAddrDict = leadData[1]\n",
    "leadTaxData = np.array(leadData[2])\n",
    "classLabl = np.array(leadData[3]).astype(int)\n",
    "\n",
    "#numnpi = leadTaxData[:,0]\n",
    "feat1 = np.sum(leadTaxData[:,pharmIndx],axis=1)[:,np.newaxis]\n",
    "feat2 = np.sum(leadTaxData[:,nonpharmIndx],axis=1)[:,np.newaxis]\n",
    "\n",
    "print(classLabl.shape,leadTaxData.shape,feat1.shape,feat2.shape,np.power(leadTaxData,2).shape)\n",
    "\n",
    "leadTaxData = np.hstack((leadTaxData,feat1,feat2,np.power(leadTaxData,2)))\n",
    "print(leadTaxData.shape)\n",
    "#sm=np.sum(leadTaxData,axis=0)\n",
    "#idx = np.array([i for i in range(len(sm)) if sm[i]>=15000])\n",
    "\n",
    "dlen = len(leadTaxData)\n",
    "\n",
    "rIndx = np.array([i for i in range(dlen)])\n",
    "\n",
    "np.random.shuffle(rIndx)\n",
    "\n",
    "leadTaxData_shuffled = leadTaxData[rIndx]\n",
    "classLabl1 = classLabl[rIndx]\n",
    "\n",
    "ri = (classLabl==1).nonzero()[0]\n",
    "ri=ri[:int(len(ri)*0.65)].tolist()\n",
    "nri = (classLabl==0).nonzero()[0].tolist()\n",
    "rdx = np.array(ri+nri)\n",
    "print(rdx.shape)\n",
    "\n",
    "print(leadTaxData_shuffled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "rIndx is the array of indices [0,1,2,...,28k] i.e. as long as number of rows in the lead dataset.  \n",
    "\n",
    "rIndx is shuffled such that the row indices get shuffled\n",
    "\n",
    "we then use the shuffled indices to shuffle the lead data set\n",
    "\n",
    "the leadAddrDict has indices as keys and values as addresses. Due to shuffling, the access has to be done using the shuffled indices list (rIndx). The value of the list has to be mapped to it's indices to get the corresponding address from leadAddrDict\n",
    "\n",
    "to do that, take row index from dataset. use that to get value in rIndx. Use that value of rIndx as the key for leadAddrDict to get the value of the lead address \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.delete(leadTaxData_shuffled,[3],axis=1)\n",
    "#print(leadTaxData_shuffled.shape)\n",
    "\n",
    "train_data = leadTaxData_shuffled.astype(float)[rdx]\n",
    "print(train_data.shape)\n",
    "train_data_class_labl = classLabl1[rdx]\n",
    "\n",
    "#print(list(train_data[0:20,:]))\n",
    "#print(train_data.shape[1])\n",
    "#print(np.sum(train_data[:,0]**2)**(1/2),np.linalg.norm(train_data,axis=0)[1])\n",
    "#train_data=train_data/np.linalg.norm(train_data,axis=0)\n",
    "#print(train_data.shape)\n",
    "#print(np.amax(train_data[:,0]))\n",
    "\n",
    "print((np.std(train_data[:,1])))\n",
    "\n",
    "for i in range(train_data.shape[1]):\n",
    "    tmax=np.amax(train_data[:,i])\n",
    "    tmin=np.amin(train_data[:,i])\n",
    "    if(tmax-tmin !=0.0):\n",
    "        train_data[:,i] = (train_data[:,i] - tmin)/(tmax - tmin)\n",
    "        train_data[:,i] = (train_data[:,i] - np.mean(train_data[:,i]))/np.std(train_data[:,i])\n",
    "\n",
    "'''\n",
    "pca=PCA(n_components=10)\n",
    "pca.fit(train_data.T)\n",
    "train_data=pca.components_.T\n",
    "print(train_data.shape)\n",
    "'''\n",
    "#print(np.sum(train_data,axis=0).tolist())\n",
    "#cross_val_data = leadTaxData_shuffled[20000:,:]\n",
    "#cross_val_data_labl = classLabl[20000:]\n",
    "#print(cross_val_data_labl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LR sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step_size = 0.02\n",
    "logreg = linear_model.LogisticRegression(C=1e3,max_iter=400,solver='saga')\n",
    "logreg.fit(train_data,train_data_class_labl.astype(int))\n",
    "predicted_labl = logreg.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg.coef_\n",
    "100*sum(abs(predicted_labl-train_data_class_labl.astype(int)))/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = [1 if predicted_labl[i]==train_data_class_labl[i].astype(int) else 0 for i in range(predicted_labl.shape[0])]\n",
    "TP.count(1)/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_false = []\n",
    "for i, l in enumerate(train_data_class_labl):\n",
    "   if l.astype(int) == 1:\n",
    "       if predicted_labl[i] == 1:\n",
    "           correct_false.append(\"True Positive\")\n",
    "       else:\n",
    "           correct_false.append(\"True Negative\")\n",
    "\n",
    "   else:\n",
    "       if predicted_labl[i] == 0:\n",
    "           correct_false.append(\"False Negative\")\n",
    "       else:\n",
    "           correct_false.append(\"False Positive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([c for c in correct_false if c =='True Positive']) / len([c for c in correct_false if 'True' in c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([c for c in correct_false if c =='False Negative']) / len([c for c in correct_false if 'False' in c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmObj = SVC(kernel='poly',degree=11)\n",
    "svmObj.fit(train_data,train_data_class_labl.astype(int))\n",
    "predicted_labl = svmObj.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Combdata=[]\n",
    "for i in range(len(train_data)):\n",
    "    Combdata.append(LabeledPoint(train_data_class_labl[i].astype(int),train_data[i]))\n",
    "\n",
    "Combdata=np.array(Combdata)    \n",
    "#Combdata = LabeledPoint(train_data_class_labl.astype(int),train_data)\n",
    "\n",
    "lrm = LogisticRegressionWithSGD.train(sc.parallelize(Combdata),iterations=5000)\n",
    "predicted_labl = lrm.predict(sc.parallelize(train_data)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = [1 if predicted_labl[i]==train_data_class_labl[i].astype(int) else 0 for i in range(len(predicted_labl))]\n",
    "TP.count(1)/len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.hstack((train_data,train_data_class_labl.reshape((train_data.shape[0],1)).astype(int)))\n",
    "\n",
    "layers = [train_data.shape[1],int(3*train_data.shape[1]/4),int(train_data.shape[1]/4),int(train_data.shape[1]/8),1]\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "model = trainer.fit(train_data)\n",
    "\n",
    "result = model.transform(train_data)\n",
    "\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#Just checking which feature has been assigned the largest weight\n",
    "###\n",
    "L=[]\n",
    "#L.append([logreg.coef_[0,0],\"Np\"])\n",
    "#L.append([logreg.coef_[0,1],\"tsp\"])\n",
    "#L.append([logreg.coef_[0,2],\"rem\"])\n",
    "taxdictKey = list(taxdict.keys())\n",
    "taxdictKey.sort()\n",
    "for i in range(0,241):\n",
    "    L.append([logreg.coef_[0,i],taxdictKey[i]])\n",
    "#sorted(L)\n",
    "#print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(100*sum(abs(predicted_labl-train_data_class_labl.astype(int)))/len(train_data))\n",
    "100*sum(abs(predicted_labl-train_data_class_labl.astype(int)))/len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### K-MEANS ####\n",
    "kmeans = KMeans(n_clusters=25,random_state=0).fit(train_data)\n",
    "\n",
    "uniq, cnt = np.unique(kmeans.labels_,return_counts =True)\n",
    "clusCount = dict(zip(uniq,cnt))\n",
    "\n",
    "print(clusCount)\n",
    "print(kmeans.labels_.shape)\n",
    "print(train_data[:,0].shape)\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=plt.axes(projection='3d')\n",
    "ax.scatter3D(train_data[:,0],train_data[:,1],train_data[:,2],marker='o')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "\"\"\"Get Addresses in the clusters\"\"\"\n",
    "clusAddrDict={k:[] for k in kmeans.labels_}\n",
    "plotDict={}\n",
    "addr_match_dict={}\n",
    "for i in range(dlen):\n",
    "    clusAddrDict[ kmeans.labels_[i] ].append( (leadAddrDict[ rIndx[i] ],leadTaxData_shuffled[ rIndx[i] ].tolist()) )\n",
    "\n",
    "    #plotDict[ leadAddrDict[ rIndx[i] ] ] = leadTaxData_shuffled[ rIndx[i] ]\n",
    "\n",
    "    addr_match_dict[ leadAddrDict[ rIndx[i] ] ] = rIndx[i]\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "#print(kmeans.cluster_centers_[1])\n",
    "with open('All_Lead_Data.json','w') as p:\n",
    "    json.dump({str(k):v for k,v in clusAddrDict.items()},p,indent=1)\n",
    "\n",
    "maxlabl = max(clusCount.items(), key=operator.itemgetter(1))[0]\n",
    "print(maxlabl,clusCount[maxlabl])\n",
    "\n",
    "txd=list(taxdict.keys())\n",
    "txd.sort()\n",
    "print(txd)\n",
    "featureVars = [\"NumPracAddr\",\"TSP Status\",\"Remedy Status\",]+txd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "num_clusters=25\n",
    "for i in range(num_clusters):\n",
    "\tplotDict={k[0]:k[1] for k in clusAddrDict[i]}\n",
    "\ttxd=np.array(txd)\n",
    "\twith codecs.open('clusterCSVs\\cluster_%s.csv'%(i),'wb','utf-8') as csvfile:\n",
    "\t\twrtvals=csv.DictWriter(csvfile,fieldnames=[\"SubstreetID\",\"Taxonomy Classes\"])\n",
    "\t\twrtvals.writeheader()\n",
    "\n",
    "\t\tfor k,v in plotDict.items():\n",
    "\t\t\t#wrtvals.writerow\n",
    "\t\t\tval=v[3:]\n",
    "\t\t\tvalind = (val!=0).nonzero()[0]\n",
    "\t\t\ttxo=txd[valind]\n",
    "\t\t\tnonzero_txo = val[valind]\n",
    "\t\t\tres=\"NumPracAddr (\"+str(v[0])+\"),\\n\"+\"TSP Status (\"+str(v[1])+\"),\\n\"+\"Remedy Status (\"+str(v[2])+\"),\\n\"\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(len(txo)):\n",
    "\t\t\t\tres+=(txo[i]+\"(\"+str(nonzero_txo[i])+\"),\\n\")\n",
    "\t\t\tif(k=='unknown_address'):\n",
    "\t\t\t\tprint(k)\n",
    "\t\t\t\n",
    "\t\t\twrtvals.writerow({\n",
    "\t\t\t\t\"SubstreetID\":k,\n",
    "\t\t\t\t\"Taxonomy Classes\":res\n",
    "\t\t\t\t})\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxlablIndx = (kmeans.labels_==maxlabl).nonzero()[0]\n",
    "#print(maxlablIndx)\n",
    "\n",
    "\n",
    "'''\n",
    "leadTaxData_shuffled_maxclus=leadTaxData_shuffled[maxlablIndx]\n",
    "\n",
    "kmeans1 = KMeans(n_clusters=25,random_state=0).fit(leadTaxData_shuffled_maxclus)\n",
    "\n",
    "u,c=np.unique(kmeans1.labels_,return_counts=True)\n",
    "clusCount_maxlabl = dict(zip(u,c))\n",
    "\n",
    "\n",
    "#print(kmeans1.labels_)\n",
    "print(clusCount_maxlabl)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qTestSubstreets = \"\"\"match (n:npi)-[:business_address]->(s:substreet)\n",
    "where n.ind_or_org='2' with s,n,rand() as r \n",
    "return distinct s.substreetId \n",
    "limit 100\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lead_substreetid =np.unique( np.squeeze(np.array((db.query(qTestSubstreets)))) , axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "l=len(taxdict)\n",
    "test_company_details=[]\n",
    "testaddrDict={}\n",
    "for substrid in test_lead_substreetid:\n",
    "    if(substrid!=None):\n",
    "        c_d=l*[0]\n",
    "        Q1 = qNpiTax%(substrid,substrid)\n",
    "        Q2 = qNumPracAddr%(substrid)\n",
    "        testaddrDict[i]=substrid\n",
    "\n",
    "        NpiTax = list(db.query(Q1))\n",
    "        NumPracAddr = db.query(Q2)[0]\n",
    "        for k in NpiTax:\n",
    "            c_d[ taxdict[ k[0] ] ]=k[1]\n",
    "\n",
    "        #feature_combined = np.array(NumPracAddr+list(lead_substreetid[i,1:3].astype(int))+c_d)\n",
    "        feature_combined = np.array(NumPracAddr+c_d)\n",
    "        test_company_details.append(feature_combined)\n",
    "        i+=1\n",
    "        print(100*(i/len(test_lead_substreetid)),end=\"\\r\")\n",
    "        #if(i==5):break\n",
    "\n",
    "test_company_details=np.array(test_company_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_company_details.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1 = np.sum(test_company_details[:,pharmIndx],axis=1)[:,np.newaxis]\n",
    "feat2 = np.sum(test_company_details[:,nonpharmIndx],axis=1)[:,np.newaxis]\n",
    "print(feat1.shape)\n",
    "\n",
    "test_company_details1 = np.hstack((test_company_details,feat1,feat2,np.power(test_company_details,2)))\n",
    "\n",
    "print(test_company_details1.shape,np.power(test_company_details1,2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print((np.std(test_company_details1[:,1])))\n",
    "\n",
    "for i in range(test_company_details1.shape[1]):\n",
    "    tmax=np.amax(test_company_details1[:,i])\n",
    "    tmin=np.amin(test_company_details1[:,i])\n",
    "    if(tmax-tmin !=0.0):\n",
    "        test_company_details1[:,i] = (test_company_details1[:,i] - tmin)/(tmax - tmin)\n",
    "        test_company_details1[:,i] = (test_company_details1[:,i] - np.mean(test_company_details1[:,i]))/np.std(test_company_details1[:,i])\n",
    "\n",
    "'''\n",
    "pca=PCA(n_components=10)\n",
    "pca.fit(train_data.T)\n",
    "train_data=pca.components_.T\n",
    "print(train_data.shape)\n",
    "'''\n",
    "#print(np.sum(train_data,axis=0).tolist())\n",
    "#cross_val_data = leadTaxData_shuffled[20000:,:]\n",
    "#cross_val_data_labl = classLabl[20000:]\n",
    "#print(cross_val_data_labl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labl = logreg.predict(test_company_details1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp=test_lead_substreetid[(predicted_labl==1).nonzero()[0]]\n",
    "rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xyz.pickle','wb') as handle:\n",
    "    pk.dump(rp,handle,protocol=pk.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
